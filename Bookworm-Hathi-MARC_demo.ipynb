{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a quick demo of what sort of data I'm pulling out with Hathi-Specific features of the Bookworm-MARC library.\n",
    "\n",
    "\n",
    "First just some basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pymarc\n",
    "import random\n",
    "import json\n",
    "from bookwormMARC.bookwormMARC import parse_record\n",
    "from bookwormMARC.hathi_methods import hathi_record_yielder\n",
    "\n",
    "import bookwormMARC\n",
    "import sys\n",
    "import os\n",
    "from collections import defaultdict\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('AC', '109', '.A42'), ('AC', '109', '.A42'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookwormMARC.bookwormMARC.LCCallNumber(\"AC109 .A42\").return_range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000000007'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"7\".zfill(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-e4e3fca7d1ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "int(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example output\n",
    "\n",
    "Here is an example of the output of this script on Hathi books: 5 randomly selected records from the first 50000 or so in the DPLA dump. This is usually, note, more than 5 *items*: Hathi groups multiple items into a single record.\n",
    "\n",
    "Note that we're using a custom superset of the pymarc.Record class called `BRecord`. This adds a number of functions that make it easier--for instance--to pull out a dictionary with the categories that may be useful for Bookworms in a variety of ways.\n",
    "\n",
    "Each of the keys here is something that might make sense to chart or analyze. We want to know the scanner so that we can see if there are OCR effects or something that might be relevant. We want the library so we can see how shifting library composition affects time series. It might make sense to build up miniature bookworms for particular authors, or publishers, etc.\n",
    "\n",
    "To start with, I just print four random records from the first 500 or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bookwormMARC.bookwormMARC import BRecord\n",
    "\n",
    "n=0\n",
    "for rec in all_files:\n",
    "    if random.random()>.01:\n",
    "        continue\n",
    "    for entry in rec.hathi_bookworm_dicts():\n",
    "        # Pretty print the dictionary entry.\n",
    "        print json.dumps(entry,sort_keys=True, indent=2, separators=(',', ': ') )\n",
    "        print \"\"\n",
    "    n+=1\n",
    "    if n>4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Experimental: testing the goodness of record 043 codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    all_files = hathi_record_yielder()\n",
    "    knowledge = open(\"/drobo/knowledge_directions.tsv\",\"w\")\n",
    "\n",
    "    for record in all_files:\n",
    "        if record['043'] is not None:\n",
    "            try:\n",
    "                dicto = record.bookworm_dict()\n",
    "                subjects = dicto['subject_places']\n",
    "                p1 = record.first_place()\n",
    "                cntry = dicto['cntry']\n",
    "                year = dicto['date']\n",
    "                for subject in subjects:\n",
    "                    knowledge.write(\"\\t\".join(map(str,[subject,p1,year,cntry,record['001'].value(),dicto['title'].encode(\"utf-8\")]))+ \"\\n\")\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The world of available fields\n",
    "\n",
    "The fields that appear in more than 10% of a randomly selected subset of records. They include the control fields; author and title information; and some more esoteric things including county of study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "n = 0\n",
    "global_counts = defaultdict(int)\n",
    "\n",
    "for record in all_files:\n",
    "    if random.random() >.2:\n",
    "        continue\n",
    "    already_seen = set([])\n",
    "    n+=1\n",
    "    from collections import defaultdict\n",
    "    for dicto in record.as_dict()['fields']:\n",
    "        name = dicto.keys()[0]\n",
    "        if 'subfields' in dicto[name]:\n",
    "            for subfield in dicto[name]['subfields']:\n",
    "                tupo = (name,subfield.keys()[0])\n",
    "        else:\n",
    "            tupo = (name,None)\n",
    "        if not tupo in already_seen:\n",
    "            global_counts[tupo] +=1\n",
    "            already_seen.add(tupo)\n",
    "    if n > 10000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [((k,v),count) for ((k,v),count) in global_counts.iteritems()]\n",
    "a.sort()\n",
    "for elem in a:\n",
    "    if elem[1] > 1000:\n",
    "        print elem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better years\n",
    "\n",
    "One of the big things I've noticed is that the 974 field has better year information than the record information, such as individual fields. \n",
    "\n",
    "The following block shows that something like 1 in 3 items, in about one in ten records, have a different entry in the 974y field from the native date field. That suggests huge possibilities for improving dates if we're not already using the 974y fields: I suspect we are not based on the serial volumes that include 974y fields I see in the online browser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "records = 0\n",
    "diff_records = 0\n",
    "items = 0\n",
    "diff_items = 0\n",
    "date_diffs = collections.defaultdict(int)\n",
    "for rec in all_files:\n",
    "    if random.random() > .01:\n",
    "        # Print just one in one hundred files each time for debugging\n",
    "        continue\n",
    "    records += 1\n",
    "    line_counted = False\n",
    "    for dicto in rec.hathi_bookworm_dicts():\n",
    "        try:\n",
    "            if dicto[\"item_date\"] != dicto[\"record_date\"]:\n",
    "                date_diffs[(dicto[\"item_date\"],dicto[\"record_date\"])] += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if records>1000:\n",
    "        break\n",
    "print \"%i out of %i records and %i out of %i items have differing dates\" %(diff_records,records,diff_items,items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing differences in dates between 974 and the main MARC record\n",
    "\n",
    "]The most common pattern is that I'm replacing a \"None\" value with an actual year, or vice versa. It would be wise to see if there isn't sometimes a better solution than the Nones for the original fields. (Eg; am I overrelying on F008?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattened = sorted([(-val,f008,f974,val) for ((f974,f008),val) in date_diffs.iteritems()])\n",
    "flattened[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at only places where we have years, most are the realm of reasonableness here.\n",
    "(With just 1000 examples, I'm certainly getting a lot of repeat entries.)\n",
    "\n",
    "There are, though, a number of places where f974 instates an earlier entry than does the native date field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattened = sorted([(-val,f008,f974,val) for ((f974,f008),val) in date_diffs.iteritems() \n",
    "                    if f974 is not None and f008 is not None])\n",
    "flattened[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look to see what those are. Here are thirty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "records = 0\n",
    "for rec in all_files:\n",
    "    if random.random() > .5:\n",
    "        # Print just one in two for debugging\n",
    "        continue\n",
    "    rec.__class__ = BRecord\n",
    "    for field in rec.get_fields('974'):\n",
    "        items += 1\n",
    "        if str(field['y']) != str(rec.date()) and field['y'] is not None and rec.date() is not None:\n",
    "            if int(field['y']) < int(rec.date()):\n",
    "                records += 1\n",
    "                print rec\n",
    "    if records>30:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic problem in all of these seems to be that in the original record, field 260c and field 008 disagree on the date. Pymarc prefers 260 in these cases; Zephir prefers field 008. Fair enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for rec in all_files:\n",
    "    dicto =  rec.bookworm_dict()\n",
    "    try:\n",
    "        if \"Wien\" in dicto['first_place']:\n",
    "            if dicto['cntry'] != 'au ':\n",
    "                break\n",
    "    except KeyError:\n",
    "        continue\n",
    "print (dicto['cntry'],dicto['first_place'])\n",
    "a = bookwormMARC.bookwormMARC.F008(rec)\n",
    "print a.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rec.hathi_bookworm_dicts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "\n",
    "dictee = defaultdict(int)\n",
    "\n",
    "for rec in all_files:\n",
    "    n+=1\n",
    "    if n > 10:\n",
    "        break\n",
    "        \n",
    "    for item in rec.hathi_bookworm_dicts():\n",
    "        if item['serial_killer_guess'] != item['resource_type']:\n",
    "            print item['title']\n",
    "        try:\n",
    "            dictee[(item['serial_killer_guess'],item['resource_type'])] += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        break\n",
    "for (k,v) in dictee.iteritems():\n",
    "    print (k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "testrow = \"CLASS C - AUXILIARY SCIENCES OF HISTORY\"\n",
    "file = open(\"/drobo/hathi_metadata/vocabularies/lc_class.txt\",\"r\")\n",
    "line = line.rstrip(\"\\n\")\n",
    "line = line.rstrip(\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_row(string):\n",
    "    \"\"\"\n",
    "    Each row is 4-tuple; the level,\n",
    "    lower bound,\n",
    "    the upper bound, and the label.\n",
    "    \"\"\"\n",
    "    l1 = re.search(r\"CLASS ([A-Z]) - (\\w+)\",string)\n",
    "    if l1:\n",
    "        groups = l1.groups()\n",
    "        return (0,groups[0],groups[0],groups[1])\n",
    "    l2 = re.search(r\"Subclass ([A-Z]{1,3})\",string)\n",
    "    if l2:\n",
    "        groups = l2.groups()\n",
    "        return (1,groups[0],groups[0],None)\n",
    "    l3 = re.search(r'([A-Z]+)\\(?(\\d+(?:\\.[A-Z]?\\d*)?)\\)?-?\\(?(\\d+(?:\\.[A-Z]?\\d*)?)?\\)?(\\t+)(.*)',string)\n",
    "    if l3:\n",
    "        groups = list(l3.groups())\n",
    "        if groups[2] is None:\n",
    "            groups[2] = groups[1]\n",
    "        return (1 + len(groups[3]),groups[1],groups[2],groups[4])\n",
    "\n",
    "line = file.readline()\n",
    "if line ==\"\\n\": line = file.readline()\n",
    "\n",
    "print line\n",
    "print parse_row(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
