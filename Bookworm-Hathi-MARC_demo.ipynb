{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a quick demo of what sort of data I'm pulling out with Hathi-Specific features of the Bookworm-MARC library.\n",
    "\n",
    "\n",
    "First just some basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymarc\n",
    "import random\n",
    "import json\n",
    "from bookwormMARC.bookwormMARC import parse_record\n",
    "from bookwormMARC.hathi_methods import hathi_record_yielder\n",
    "\n",
    "import bookwormMARC\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'm using a generator that makes it possible to parse the dumps one entry at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing new XML file dpla_full_20160501_01.xmlParsing new XML file dpla_full_20160501_02.xmlParsing new XML file dpla_full_20160501_03.xmlParsing new XML file dpla_full_20160501_04.xmlParsing new XML file dpla_full_20160501_05.xmlParsing new XML file dpla_full_20160501_06.xmlParsing new XML file dpla_full_20160501_07.xmlParsing new XML file dpla_full_20160501_08.xmlParsing new XML file dpla_full_20160501_09.xmlParsing new XML file dpla_full_20160501_10.xml"
     ]
    }
   ],
   "source": [
    "all_files = hathi_record_yielder()\n",
    "knowledge = open(\"/drobo/knowledge_directions.tsv\",\"w\")\n",
    "\n",
    "for record in all_files:\n",
    "    if record['043'] is not None:\n",
    "        try:\n",
    "            dicto = record.bookworm_dict()\n",
    "            subjects = dicto['subject_places']\n",
    "            p1 = record.first_place()\n",
    "            cntry = dicto['cntry']\n",
    "            year = dicto['date']\n",
    "            for subject in subjects:\n",
    "                knowledge.write(\"\\t\".join(map(str,[subject,p1,year,cntry,record['001'].value(),dicto['title'].encode(\"utf-8\")]))+ \"\\n\")\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cataloging_source': u' ',\n",
       " 'cntry': u'dcu',\n",
       " 'date': 1970,\n",
       " 'first_place': u'Washington,',\n",
       " 'first_publisher': u'U.S. Govt. Print. Off.,',\n",
       " 'government_document': u'f',\n",
       " 'language': u'eng',\n",
       " 'lc0': 'K',\n",
       " 'lc1': 'KF',\n",
       " 'lc2': '9219',\n",
       " 'lc_class_from_lc': True,\n",
       " 'marc_record_created': u'1984-01-23',\n",
       " 'subject_places': [u'n-us---'],\n",
       " 'title': u'Working papers of the National Commission on Reform of Federal Criminal Laws relating to the Study draft of the new Federal criminal code.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.bookworm_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "global_counts = defaultdict(int)\n",
    "\n",
    "for record in all_files:\n",
    "    if random.random() >.2:\n",
    "        continue\n",
    "    already_seen = set([])\n",
    "    n+=1\n",
    "    from collections import defaultdict\n",
    "    for dicto in record.as_dict()['fields']:\n",
    "        name = dicto.keys()[0]\n",
    "        if 'subfields' in dicto[name]:\n",
    "            for subfield in dicto[name]['subfields']:\n",
    "                tupo = (name,subfield.keys()[0])\n",
    "        else:\n",
    "            tupo = (name,None)\n",
    "        if not tupo in already_seen:\n",
    "            global_counts[tupo] +=1\n",
    "            already_seen.add(tupo)\n",
    "    if n > 10000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [((k,v),count) for ((k,v),count) in global_counts.iteritems()]\n",
    "a.sort()\n",
    "for elem in a:\n",
    "    if elem[1] > 1000:\n",
    "        print elem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the fields that appear in more than 10% of a randomly selected subset of records. They include the control fields; author and title information; and some more esoteric things including county of study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for record in all_files:\n",
    "    try:\n",
    "        print record['043']['a']\n",
    "        print record.title()\n",
    "        break\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This would be easier to demo if this could be loaded over the web. But that apparently will take Python 3.\n",
    "#import urllib2\n",
    "\n",
    "hathi_records = tarfile.open(\"dpla_full_20160501.tar.gz\")\n",
    "\n",
    "def record_yielder(files = hathi_records):\n",
    "    for file in files:\n",
    "        if file.name.endswith(\".xml\"):\n",
    "            sys.stderr.write(\"Parsing new XML file \" + file.name)\n",
    "            records = pymarc.parse_xml_to_array(files.extractfile(file))\n",
    "            for record in records:\n",
    "                yield record\n",
    "                \n",
    "all_files = record_yielder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it requires a fair amount of dedicated RAM, since it blocks off huge section of MARC records at a time. \n",
    "It also takes a while to load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, for example, is an early record in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for record in all_files:\n",
    "    print record\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example output\n",
    "\n",
    "Here is an example of the output of this script on Hathi books: 5 randomly selected records from the first 50000 or so in the DPLA dump. This is usually, note, more than 5 *items*: Hathi groups multiple items into a single record.\n",
    "\n",
    "Note that we're using a custom superset of the pymarc.Record class called `BRecord`. This adds a number of functions that make it easier--for instance--to pull out a dictionary with the categories that may be useful for Bookworms in a variety of ways.\n",
    "\n",
    "Each of the keys here is something that might make sense to chart or analyze. We want to know the scanner so that we can see if there are OCR effects or something that might be relevant. We want the library so we can see how shifting library composition affects time series. It might make sense to build up miniature bookworms for particular authors, or publishers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bookwormMARC.bookwormMARC import BRecord\n",
    "\n",
    "n=0\n",
    "for rec in all_files:\n",
    "    # Reclass so we can use the extended methods.\n",
    "    rec.__class__=bookwormMARC.bookwormMARC.BRecord\n",
    "    for entry in rec.hathi_bookworm_dicts():\n",
    "        # Pretty print the dictionary entry.\n",
    "        print json.dumps(entry,sort_keys=True, indent=2, separators=(',', ': ') )\n",
    "        print \"\"\n",
    "    n+=1\n",
    "    if n>4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better years\n",
    "\n",
    "One of the big things I've noticed is that the 974 field has better year information than the record information, such as individual fields. \n",
    "\n",
    "The following block shows that something like 1 in 3 items, in about one in ten records, have a different entry in the 974y field from the native date field. That suggests huge possibilities for improving dates if we're not already using the 974y fields: I suspect we are not based on the serial volumes that include 974y fields I see in the online browser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "records = 0\n",
    "diff_records = 0\n",
    "items = 0\n",
    "diff_items = 0\n",
    "date_diffs = dict()\n",
    "for rec in all_files:\n",
    "    if random.random() > .01:\n",
    "        # Print just one in one hundred files each time for debugging\n",
    "        continue\n",
    "    rec.__class__ = BRecord\n",
    "    records += 1\n",
    "    line_counted = False\n",
    "    for field in rec.get_fields('974'):\n",
    "        items += 1\n",
    "        if str(field['y']) != str(rec.date()):\n",
    "            try:\n",
    "                date_diffs[(str(field['y']), str(rec.date()))] += 1\n",
    "            except: \n",
    "                date_diffs[(str(field['y']), str(rec.date()))] = 1\n",
    "            if not line_counted:\n",
    "                diff_records += 1\n",
    "                line_counted = True\n",
    "            diff_items +=1\n",
    "    if records>1000:\n",
    "        break\n",
    "print \"%i out of %i records and %i out of %i items have differing dates\" %(diff_records,records,diff_items,items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing differences in dates between 974 and the main MARC record\n",
    "\n",
    "]The most common pattern is that I'm replacing a \"None\" value with an actual year, or vice versa. It would be wise to see if there isn't sometimes a better solution than the Nones for the original fields. (Eg; am I overrelying on F008?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattened = sorted([(-val,f008,f974,val) for ((f974,f008),val) in date_diffs.iteritems()])\n",
    "flattened[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at only places where we have years, most are the realm of reasonableness here.\n",
    "(With just 1000 examples, I'm certainly getting a lot of repeat entries.)\n",
    "\n",
    "There are, though, a number of places where f974 instates an earlier entry than does the native date field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattened = sorted([(-val,f008,f974,val) for ((f974,f008),val) in date_diffs.iteritems() \n",
    "                    if f974 != \"None\" and f008 != \"None\"])\n",
    "flattened[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look to see what those are. Here are thirty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "records = 0\n",
    "for rec in all_files:\n",
    "    if random.random() > .5:\n",
    "        # Print just one in two for debugging\n",
    "        continue\n",
    "    rec.__class__ = BRecord\n",
    "    for field in rec.get_fields('974'):\n",
    "        items += 1\n",
    "        if str(field['y']) != str(rec.date()) and field['y'] is not None and rec.date() is not None:\n",
    "            if int(field['y']) < int(rec.date()):\n",
    "                records += 1\n",
    "                print rec\n",
    "    if records>30:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic problem in all of these seems to be that in the original record, field 260c and field 008 disagree on the date. Pymarc prefers 260 in these cases; Zephir prefers field 008. Fair enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    rec = parse_record(hathi_records.readline().strip(\",\"))\n",
    "    dicto =  rec.bookworm_dict()\n",
    "    \n",
    "    try:\n",
    "        if \"Wien\" in dicto['first_place']:\n",
    "            if dicto['cntry'] != 'au ':\n",
    "                break\n",
    "    except KeyError:\n",
    "        continue\n",
    "print (dicto['cntry'],dicto['first_place'])\n",
    "a = bookwormMARC.bookwormMARC.F008(rec)\n",
    "print a.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    rec = parse_record(hathi_records.readline().strip(\",\"))\n",
    "    if '045' in rec:\n",
    "        print rec['045'].value()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print rec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
